{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "#import sys\n",
    "#sys.path.append('..')\n",
    "#from utils import get_train_data, create_preprocessor, submit_test, train_test_split_temporal, get_feature_lists\n",
    "\n",
    "#X, y = get_train_data()\n",
    "data = pd.read_parquet(\"../data/train.parquet\")\n",
    "_target_column_name = \"log_bike_count\"\n",
    "\n",
    "data['date'] = pd.to_datetime(data['date'], dayfirst=True)  # Ensure the date format is parsed correctly\n",
    "\n",
    "y = data[_target_column_name].values\n",
    "X = data.drop([_target_column_name, \"bike_count\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split_temporal(X, y, delta_threshold=\"30 days\"):\n",
    "    \"\"\"\n",
    "    Split the data into training and validation sets based on a temporal cutoff.\n",
    "    Args:\n",
    "        X (pd.DataFrame): Features with a `date` column.\n",
    "        y (pd.Series): Target variable.\n",
    "        delta_threshold (str): Time delta defining the validation cutoff.\n",
    "    Returns:\n",
    "        Tuple: X_train, y_train, X_valid, y_valid\n",
    "    \"\"\"\n",
    "    cutoff_date = X[\"date\"].max() - pd.Timedelta(delta_threshold)\n",
    "    mask = (X[\"date\"] <= cutoff_date)\n",
    "    X_train, X_valid = X.loc[mask], X.loc[~mask]\n",
    "    y_train, y_valid = y[mask], y[~mask]\n",
    "    return X_train, X_valid, y_train, y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jours_feries_france import JoursFeries\n",
    "import os\n",
    "\n",
    "def add_arrondissement(df):\n",
    "    \"\"\"\n",
    "    Adds district information to the DataFrame based on a predefined dictionary.\n",
    "    \"\"\"\n",
    "    district_mapping = {\n",
    "        '28 boulevard Diderot': 12,\n",
    "        '39 quai François Mauriac': 13,\n",
    "        \"18 quai de l'Hôtel de Ville\": 4,\n",
    "        'Voie Georges Pompidou': 4,\n",
    "        '67 boulevard Voltaire SE-NO': 11,\n",
    "        'Face au 48 quai de la marne': 19,\n",
    "        \"Face 104 rue d'Aubervilliers\": 19,\n",
    "        'Face au 70 quai de Bercy': 12,\n",
    "        '6 rue Julia Bartet': 16,\n",
    "        \"Face au 25 quai de l'Oise\": 19,\n",
    "        '152 boulevard du Montparnasse': 14,\n",
    "        'Totem 64 Rue de Rivoli': 1,\n",
    "        'Pont des Invalides S-N': 7,\n",
    "        'Pont de la Concorde S-N': 7,\n",
    "        'Pont des Invalides N-S': 7,\n",
    "        'Face au 8 avenue de la porte de Charenton': 12,\n",
    "        'Face au 4 avenue de la porte de Bagnolet': 20,\n",
    "        'Pont Charles De Gaulle': 13,\n",
    "        '36 quai de Grenelle': 15,\n",
    "        \"Face au 40 quai D'Issy\": 15,\n",
    "        'Pont de Bercy': 12,\n",
    "        '38 rue Turbigo': 3,\n",
    "        \"Quai d'Orsay\": 7,\n",
    "        '27 quai de la Tournelle': 5,\n",
    "        \"Totem 85 quai d'Austerlitz\": 13,\n",
    "        'Totem Cours la Reine': 8,\n",
    "        'Totem 73 boulevard de Sébastopol': 1,\n",
    "        '90 Rue De Sèvres': 7,\n",
    "        '20 Avenue de Clichy': 17,\n",
    "        '254 rue de Vaugirard': 15\n",
    "    }\n",
    "    # Apply the district mapping\n",
    "    df = df.copy()\n",
    "    df['arrondissement'] = df['site_name'].map(district_mapping)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# Import the bank holidays in France for 2020 and 2021\n",
    "holidays_2020 = JoursFeries.for_year(2020)\n",
    "holidays_2021 = JoursFeries.for_year(2021)\n",
    "\n",
    "# Create lists of dates from each dictionary\n",
    "dates_2020 = list(holidays_2020.values())\n",
    "dates_2021 = list(holidays_2021.values())\n",
    "\n",
    "# Create DataFrame with all dates\n",
    "all_dates = dates_2020 + dates_2021\n",
    "bank_holidays_df = pd.DataFrame(all_dates, columns=[\"date\"])\n",
    "bank_holidays_df[\"date\"] = pd.to_datetime(bank_holidays_df[\"date\"])\n",
    "\n",
    "# Add a new column \"is_bank_holiday\" to the data dataframe\n",
    "def is_holidays(df):\n",
    "    df[\"is_bank_holiday\"] = df[\"date\"].dt.date.isin(bank_holidays_df[\"date\"].dt.date).astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Dividing a day into 4 relevant sections\n",
    "def assign_time_interval(hour):\n",
    "    if 5 <= hour < 9:\n",
    "        return 'morning'\n",
    "    elif 9 <= hour < 15:\n",
    "        return 'working_hours'\n",
    "    elif 15 <= hour < 20:\n",
    "        return 'peak_hours'\n",
    "    else:\n",
    "        return 'calm'\n",
    "\n",
    "def _encode_dates(X):\n",
    "    X = X.copy()  # modify a copy of X\n",
    "    # Encode the date information from the DateOfDeparture columns\n",
    "    X.loc[:, \"year\"] = X[\"date\"].dt.year\n",
    "    X.loc[:, \"month\"] = X[\"date\"].dt.month\n",
    "    X.loc[:, \"day\"] = X[\"date\"].dt.day\n",
    "    X.loc[:, \"weekday\"] = X[\"date\"].dt.weekday\n",
    "    X.loc[:, \"hour\"] = X[\"date\"].dt.hour\n",
    "    X['is_weekend'] = X['weekday'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "    \n",
    "    X['season'] = X['month'] % 12 // 3 # Winter=0, Spring=1, Summer=2, Fall=3\n",
    "    X['time_interval'] = X['hour'].apply(assign_time_interval)\n",
    "\n",
    "    # Cyclical encoding\n",
    "    X['hour_sin'] = np.sin(2 * np.pi * X['hour']/24)\n",
    "    X['hour_cos'] = np.cos(2 * np.pi * X['hour']/24)\n",
    "    X['day_sin'] = np.sin(2 * np.pi * X['weekday']/7)\n",
    "    X['day_cos'] = np.cos(2 * np.pi * X['weekday']/7)\n",
    "\n",
    "    # One-hot encoding time_interval\n",
    "    X = pd.get_dummies(X, columns=['time_interval'], prefix='time')\n",
    "    \n",
    "    # One-hot encoding for day_of_week. Season one-hot-encoding is extremely bad for the model!!!\n",
    "    #X = pd.get_dummies(X, columns=['weekday', 'season'], prefix=['day', 'season']) \n",
    "    X = pd.get_dummies(X, columns=['weekday'], prefix=['day'])\n",
    "    \n",
    "    # Finally we can drop the original columns from the dataframe\n",
    "    #X = X.drop(columns=[\"date\"])\n",
    "    \n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = _encode_dates(X)\n",
    "X = is_holidays(X)\n",
    "X = add_arrondissement(X)\n",
    "\n",
    "X = X.drop(columns=[\"coordinates\", \"counter_name\", \"site_name\",\n",
    "                              \"counter_installation_date\",\"counter_technical_id\",\n",
    "                              \"counter_id\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split_temporal(X, y)\n",
    "X_train = X_train.drop(columns=[\"date\"])\n",
    "X_valid = X_valid.drop(columns=[\"date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Gridsearch, CV on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 16 candidates, totalling 64 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\felix\\miniconda3\\envs\\l1-python\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [10:11:57] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 300, 'subsample': 1.0}\n",
      "Test MSE: 0.3465580611026357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\felix\\miniconda3\\envs\\l1-python\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [10:12:04] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\felix\\miniconda3\\envs\\l1-python\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [10:12:04] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "Potential solutions:\n",
      "- Use a data structure that matches the device ordinal in the booster.\n",
      "- Set the device for booster before call to inplace_predict.\n",
      "\n",
      "This warning will only be shown once.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Define the model with GPU support\n",
    "model = XGBRegressor(tree_method='hist', device='cuda', random_state=42, verbosity=1)\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 300],\n",
    "    'learning_rate': [0.01, 0.1], \n",
    "    'max_depth': [3, 7],\n",
    "    'subsample': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Perform GridSearch with verbose progress\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    cv=4,  # 4-fold cross-validation\n",
    "    scoring='neg_mean_squared_error',  # Optimize MSE\n",
    "    verbose=2,  # Verbose for GridSearchCV\n",
    "    n_jobs=-1  # Use all available cores for the CPU part of the grid search\n",
    ")\n",
    "\n",
    "# Fit the grid search\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and model\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate on the test set\n",
    "y_pred = best_model.predict(X_valid)\n",
    "test_mse = mean_squared_error(y_valid, y_pred)\n",
    "\n",
    "# Output results\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Test MSE:\", test_mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best Parameters: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 300, 'subsample': 1.0}\n",
    "Test MSE: 0.3465580611026357"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5886934686235273"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(0.34656)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submission\n",
    "file_name = \"../submissions/\" + \"XGBRegressor_gridsearch_optimum\" + \"_submission.csv\"\n",
    "\n",
    "X_test = pd.read_parquet(\"../data/final_test.parquet\")\n",
    "\n",
    "X_test = _encode_dates(X_test)\n",
    "X_test = is_holidays(X_test)\n",
    "X_test = add_arrondissement(X_test)\n",
    "\n",
    "X_test = X_test.drop(columns=[\"coordinates\", \"counter_name\", \"site_name\",\n",
    "                              \"counter_installation_date\",\"counter_technical_id\",\n",
    "                              \"counter_id\", \"date\"])\n",
    "\n",
    "\n",
    "y_predict = best_model.predict(X_test)\n",
    "\n",
    "results = pd.DataFrame(\n",
    "dict(\n",
    "    Id=np.arange(y_predict.shape[0]),\n",
    "    log_bike_count=y_predict,\n",
    "    )\n",
    ")\n",
    "results.to_csv(file_name, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM with GridCearch, CV on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 8 candidates, totalling 32 fits\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "\n",
    "# Define the model with GPU support\n",
    "model = LGBMRegressor(boosting_type='gbdt', device='gpu', random_state=42)\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 300],\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "    'max_depth': [3, 7]\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV with verbose progress\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    cv=4,  # 4-fold cross-validation\n",
    "    scoring='neg_mean_squared_error',  # Optimize MSE\n",
    "    verbose=2,  # Verbose for GridSearchCV\n",
    "    n_jobs=-1  # Use all available cores for CPU tasks\n",
    ")\n",
    "\n",
    "# Fit the grid search\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and model\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate on the test set\n",
    "y_pred = best_model.predict(X_valid)\n",
    "test_mse = mean_squared_error(y_valid, y_pred)\n",
    "\n",
    "# Output results\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Test MSE:\", test_mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submission\n",
    "file_name = \"../submissions/\" + \"lightgbm_gridsearch_optimum\" + \"_submission.csv\"x\n",
    "\n",
    "X_test = pd.read_parquet(\"../data/final_test.parquet\")\n",
    "\n",
    "X_test = _encode_dates(X_test)\n",
    "X_test = is_holidays(X_test)\n",
    "X_test = add_arrondissement(X_test)\n",
    "\n",
    "X_test = X_test.drop(columns=[\"coordinates\", \"counter_name\", \"site_name\",\n",
    "                              \"counter_installation_date\",\"counter_technical_id\",\n",
    "                              \"counter_id\", \"date\"])\n",
    "\n",
    "\n",
    "y_predict = best_model.predict(X_test)\n",
    "\n",
    "results = pd.DataFrame(\n",
    "dict(\n",
    "    Id=np.arange(y_predict.shape[0]),\n",
    "    log_bike_count=y_predict,\n",
    "    )\n",
    ")\n",
    "results.to_csv(file_name, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-0f2c66d80454>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Define the features to scale\n",
    "scale_features = ['latitude', 'longitude', 'year', 'hour_sin', 'hour_cos', 'day_sin', 'day_cos']\n",
    "\n",
    "# Copy the data to avoid altering the original\n",
    "X_train_scaled = X_train.copy()\n",
    "X_valid_scaled = X_valid.copy()\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the necessary features in X_train\n",
    "X_train_scaled[scale_features] = scaler.fit_transform(X_train[scale_features])\n",
    "\n",
    "# Transform the same features in X_test\n",
    "X_valid_scaled[scale_features] = scaler.transform(X_valid[scale_features])\n",
    "\n",
    "# Define the deep learning model\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_dim=X_train_scaled.shape[1]),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(1)  # Output layer for regression\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mse'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_scaled, y_train, validation_split=0.2, epochs=50, batch_size=512, verbose=1, shuffle=False)\n",
    "\n",
    "# Evaluate on the test set\n",
    "y_pred = model.predict(X_valid_scaled).flatten()\n",
    "test_mse = mean_squared_error(y_valid, y_pred)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "\n",
    "print(\"Test RMSE:\", test_rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-b3c3452a9f10>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Num GPUs Available: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlist_physical_devices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'GPU'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "l1-python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
