{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example notebook to show how a model pipeline can be easily executed used our custom functions. All functions are adapted to be used in files within the models folder, but the path to the data files or modules can also be adjusted to be used in any folder.\n",
    "\n",
    "**The first 3 cells and the last should not be changed. You can adapt the fourth one to any model you would want to use and apply gridsearch if necessary**\n",
    "\n",
    "- I haven't had the time to incorporate cross-validation, but I think you can easily add it here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from utils import get_train_data, create_preprocessor, submit_test, train_test_split_temporal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = get_train_data() # X comes out prepared with added columns\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split_temporal(X, y)\n",
    "\n",
    "X_train = X_train.drop(columns=[\"date\"])\n",
    "X_valid = X_valid.drop(columns=[\"date\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['site_id', 'latitude', 'longitude', 'numer_sta', 'pmer', 'tend',\n",
       "       'cod_tend', 'dd', 't', 'td', 'u', 'vv', 'ww', 'w1', 'w2', 'nbas',\n",
       "       'hbas', 'cl', 'cm', 'ch', 'pres', 'tend24', 'raf10', 'rafper', 'per',\n",
       "       'etat_sol', 'ht_neige', 'perssfrai', 'rr1', 'rr3', 'rr6', 'rr12',\n",
       "       'rr24', 'nnuage1', 'ctype1', 'hnuage1', 'is_bank_holiday', 'year',\n",
       "       'month', 'day', 'hour', 'is_weekend', 'season', 'hour_sin', 'hour_cos',\n",
       "       'day_sin', 'day_cos', 'time_calm', 'time_morning', 'time_peak_hours',\n",
       "       'time_working_hours', 'day_0', 'day_1', 'day_2', 'day_3', 'day_4',\n",
       "       'day_5', 'day_6', 'arrondissement'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.68\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Define the pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    #('preprocessor', preprocessor),# This preprocessor should always be used\n",
    "    ('regressor', GradientBoostingRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "# Train the pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the pipeline\n",
    "y_pred = pipeline.predict(X_valid)\n",
    "rmse = np.sqrt(mean_squared_error(y_valid, y_pred))\n",
    "\n",
    "print(f\"Mean Squared Error: {np.round(rmse, 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Feature  Absolute Normalized Importance\n",
      "43            hour_sin                    1.925390e+03\n",
      "44            hour_cos                    1.758393e+03\n",
      "45             day_sin                    5.667942e+00\n",
      "47           time_calm                    5.581745e-01\n",
      "46             day_cos                    3.141649e-01\n",
      "36     is_bank_holiday                    1.081119e-01\n",
      "5                 tend                    2.490901e-02\n",
      "2            longitude                    2.144225e-02\n",
      "41          is_weekend                    2.036665e-02\n",
      "57               day_6                    1.621579e-02\n",
      "48        time_morning                    1.129914e-02\n",
      "49     time_peak_hours                    1.038583e-02\n",
      "50  time_working_hours                    9.553494e-03\n",
      "42              season                    9.100285e-03\n",
      "56               day_5                    7.259015e-03\n",
      "58      arrondissement                    5.510631e-03\n",
      "25            etat_sol                    5.302973e-03\n",
      "40                hour                    5.040461e-03\n",
      "28                 rr1                    4.428216e-03\n",
      "29                 rr3                    2.020142e-03\n",
      "38               month                    1.655955e-03\n",
      "21              tend24                    1.248269e-03\n",
      "55               day_4                    1.091102e-03\n",
      "51               day_0                    7.614446e-04\n",
      "1             latitude                    4.297435e-04\n",
      "13                  w1                    4.127250e-04\n",
      "8                    t                    1.251066e-04\n",
      "12                  ww                    1.171151e-04\n",
      "32                rr24                    9.745311e-05\n",
      "22               raf10                    7.937900e-05\n",
      "10                   u                    6.301020e-05\n",
      "23              rafper                    3.111616e-05\n",
      "39                 day                    2.969195e-05\n",
      "9                   td                    1.400468e-06\n",
      "35             hnuage1                    3.263581e-07\n",
      "37                year                    2.448145e-07\n",
      "4                 pmer                    1.710005e-09\n",
      "20                pres                    1.410919e-09\n",
      "0              site_id                    3.749188e-10\n",
      "7                   dd                    0.000000e+00\n",
      "3            numer_sta                    0.000000e+00\n",
      "24                 per                    0.000000e+00\n",
      "19                  ch                    0.000000e+00\n",
      "54               day_3                    0.000000e+00\n",
      "53               day_2                    0.000000e+00\n",
      "52               day_1                    0.000000e+00\n",
      "26            ht_neige                    0.000000e+00\n",
      "6             cod_tend                    0.000000e+00\n",
      "17                  cl                    0.000000e+00\n",
      "34              ctype1                    0.000000e+00\n",
      "18                  cm                    0.000000e+00\n",
      "30                 rr6                    0.000000e+00\n",
      "11                  vv                    0.000000e+00\n",
      "31                rr12                    0.000000e+00\n",
      "33             nnuage1                    0.000000e+00\n",
      "14                  w2                    0.000000e+00\n",
      "15                nbas                    0.000000e+00\n",
      "16                hbas                    0.000000e+00\n",
      "27           perssfrai                    0.000000e+00\n"
     ]
    }
   ],
   "source": [
    "# Assuming X_train is the DataFrame used as input to the pipeline\n",
    "feature_names = X_train.columns\n",
    "\n",
    "gbr_model = pipeline.named_steps['regressor']\n",
    "\n",
    "feature_importances = gbr_model.feature_importances_\n",
    "\n",
    "# Compute the average value for each feature in X_train\n",
    "column_averages = X_train.mean()\n",
    "\n",
    "# Normalize feature importances by dividing by column averages\n",
    "normalized_importances = feature_importances / column_averages.values\n",
    "\n",
    "feature_weights = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': feature_importances,\n",
    "    'Normalized Importance': normalized_importances,\n",
    "    'Absolute Normalized Importance': np.abs(normalized_importances)\n",
    "}).sort_values(by='Absolute Normalized Importance', ascending=False)\n",
    "\n",
    "# Display the sorted DataFrame\n",
    "print(feature_weights[['Feature', 'Absolute Normalized Importance']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the preprocessor\n",
    "preprocessor = create_preprocessor(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell you can experiment with different models and try out gridsearch on different hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Error processing school holidays: name 'Path' is not defined\n"
     ]
    }
   ],
   "source": [
    "# Don't forget to nclude the pipeline and a name that will be used for the file\n",
    "submit_test(pipeline, \"GradientBoostingRegressor_default\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "l1-python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
